# RetrivalLMPapers(personnal usage)

![](https://img.shields.io/github/last-commit/Timothyxxx/RetrivalLMPapers?color=green)

Paper collections of retrieval-based(augmented) language model.

## Papers

1. **Generalization through Memorization: Nearest Neighbor Language Models.** ICLR.
   
   *Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, Mike Lewis*  [[pdf](https://arxiv.org/abs/1911.00172)], [[implementation](https://github.com/urvashik/knnlm2019.11)] 2019.11

2. **Retrieval Augmented Language Model Pre-Training.** PMLR.
   
   *Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang*  [[pdf](https://arxiv.org/abs/2002.08909)] 2020.2

3. **Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.** NIPS.
   
   *Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela*  [[pdf](https://arxiv.org/abs/2005.11401)], [[implementation](https://github.com/huggingface/transformers/blob/master/examples/rag/)], 2020.5

4. **Adaptive Semiparametric Language Models.** TACL.
   
   *Dani Yogatama, Cyprien de Masson d'Autume, Lingpeng Kong*  [[pdf](https://arxiv.org/abs/2102.02557)], 2021.02

5. **Improving language models by retrieving from trillions of tokens.** Preprint.
   
   *Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, Laurent Sifre*  [[pdf](https://arxiv.org/abs/2112.04426)], [[unofficial-implementation](https://github.com/lucidrains/RETRO-pytorch)], 2021.12

6. **LaMDA: Language Models for Dialog Applications.** Preprint.
   
   *Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, Quoc Le*  [[pdf](https://arxiv.org/abs/2201.08239)], [[blog](https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.html)], 2022.1
   - Reason first, then respond: Modular generation for knowledge-infused dialogue.
   - Internet-Augmented Dialogue Generation
   - Retrieving and reading: A comprehensive survey on open-domain question answering
   - Dense passage retrieval for open-domain question answering
   - Improving language models by retrieving from trillions of tokens
   - Leveraging passage retrieval with generative models for open domain question answering


